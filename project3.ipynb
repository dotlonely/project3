{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1523dc2",
   "metadata": {},
   "source": [
    "\n",
    "# Jupyter Notebook: Explanation of Experiments in Layman's Terms\n",
    "\n",
    "**Introduction:**\n",
    "This project is about predicting house prices using different techniques. We are going to try various methods to make our predictions more accurate and to see which approach works best. We'll begin with simpler methods and move to more advanced ones to understand their benefits and challenges.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ae254b",
   "metadata": {},
   "source": [
    "\n",
    "**Experiment 1: Basic Linear Regression**\n",
    "- **What's Happening:** In this first experiment, we use a straightforward method called linear regression. Think of it as drawing a straight line through the data to predict house prices based on a few key features (like house size and number of rooms). This experiment serves as our baseline, or starting point, to see how well a simple model can predict prices.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c2ef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing Libraries and Loading Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "# 1. Introduction: Loading dataset and providing a brief overview\n",
    "train = pd.read_csv('train.csv')\n",
    "print(\"Dataset Overview:\")\n",
    "print(train.info())\n",
    "print(train.describe())\n",
    "\n",
    "# Checking target variable (SalePrice) distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train['SalePrice'], kde=True)\n",
    "plt.title(\"Distribution of Sale Prices\")\n",
    "plt.xlabel(\"Sale Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Selecting basic features for Experiment 1\n",
    "features = ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
    "X = train[features]\n",
    "y = train['SalePrice']\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions and evaluating\n",
    "y_pred = linear_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Experiment 1 - RMSE for Linear Regression: {rmse}\")\n",
    "\n",
    "# Plotting actual vs predicted values\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actual Sale Price\")\n",
    "plt.ylabel(\"Predicted Sale Price\")\n",
    "plt.title(\"Experiment 1: Actual vs Predicted Sale Price\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3559849",
   "metadata": {},
   "source": [
    "\n",
    "**Experiment 2: Improving with More Features and Ridge Regression**\n",
    "- **What's Happening:** Here, we improve upon our first attempt by adding more features and using a different method called Ridge Regression. Instead of just drawing a straight line, we're trying to include more details to better capture relationships in the data. Ridge Regression also helps prevent overfitting, which is like making the model too focused on our specific datasetâ€”making it less useful for new, unseen data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adding new polynomial features and scaling for Ridge Regression\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_poly_scaled = scaler.fit_transform(X_poly)\n",
    "\n",
    "# Splitting data with transformed features\n",
    "X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(X_poly_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training Ridge regression model\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train_poly, y_train_poly)\n",
    "\n",
    "# Making predictions and evaluating\n",
    "y_pred_poly = ridge_model.predict(X_test_poly)\n",
    "rmse_poly = np.sqrt(mean_squared_error(y_test_poly, y_pred_poly))\n",
    "print(f\"Experiment 2 - RMSE for Ridge Regression: {rmse_poly}\")\n",
    "\n",
    "# Plotting actual vs predicted values\n",
    "plt.scatter(y_test_poly, y_pred_poly)\n",
    "plt.xlabel(\"Actual Sale Price\")\n",
    "plt.ylabel(\"Predicted Sale Price\")\n",
    "plt.title(\"Experiment 2: Actual vs Predicted Sale Price (Ridge Regression)\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d814c2",
   "metadata": {},
   "source": [
    "\n",
    "**Experiment 3: Even More Details with Lasso Regression**\n",
    "- **What's Happening:** Now, we're taking an even deeper dive by adding more features and using Lasso Regression. Lasso is similar to Ridge but it can actually remove less useful features, simplifying our model. We're experimenting to see if removing some features helps improve the accuracy of our predictions.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edff925",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adding additional relevant features and Lasso regression model\n",
    "additional_features = features + ['1stFlrSF', '2ndFlrSF', 'LotArea', 'BsmtFinSF1', 'Fireplaces']\n",
    "X_extended = train[additional_features]\n",
    "\n",
    "# Splitting data with extended feature set\n",
    "X_train_ext, X_test_ext, y_train_ext, y_test_ext = train_test_split(X_extended, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training Lasso regression model\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_train_ext, y_train_ext)\n",
    "\n",
    "# Making predictions and evaluating\n",
    "y_pred_ext = lasso_model.predict(X_test_ext)\n",
    "rmse_ext = np.sqrt(mean_squared_error(y_test_ext, y_pred_ext))\n",
    "print(f\"Experiment 3 - RMSE for Lasso Regression with adjustments: {rmse_ext}\")\n",
    "\n",
    "# Plotting actual vs predicted values\n",
    "plt.scatter(y_test_ext, y_pred_ext)\n",
    "plt.xlabel(\"Actual Sale Price\")\n",
    "plt.ylabel(\"Predicted Sale Price\")\n",
    "plt.title(\"Experiment 3: Actual vs Predicted Sale Price (Lasso Regression with Adjustments)\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a87d54",
   "metadata": {},
   "source": [
    "\n",
    "**Experiment 3 (Alternative): Using Lasso with Cross-Validation**\n",
    "- **What's Happening:** In this part, we take Lasso Regression a step further by using an automated way to find the best settings. Cross-validation means we're using different parts of the data to test and find the best balance for making predictions. This helps us avoid making mistakes like overfitting or underfitting the data.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10917f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_extended_scaled = scaler.fit_transform(X_extended)\n",
    "\n",
    "# Splitting data with extended feature set and standardized features\n",
    "X_train_ext, X_test_ext, y_train_ext, y_test_ext = train_test_split(X_extended_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Using LassoCV to automatically select the best alpha and handle convergence\n",
    "lasso_cv_model = LassoCV(alphas=[0.1, 0.5, 1.0, 5.0, 10.0], max_iter=10000, cv=5)\n",
    "lasso_cv_model.fit(X_train_ext, y_train_ext)\n",
    "\n",
    "# Getting the best alpha\n",
    "print(f\"Optimal alpha selected by LassoCV: {lasso_cv_model.alpha_}\")\n",
    "\n",
    "# Making predictions and evaluating\n",
    "y_pred_ext_cv = lasso_cv_model.predict(X_test_ext)\n",
    "rmse_ext_cv = np.sqrt(mean_squared_error(y_test_ext, y_pred_ext_cv))\n",
    "print(f\"Experiment 3 - RMSE for Lasso Regression with LassoCV: {rmse_ext_cv}\")\n",
    "\n",
    "# Plotting actual vs predicted values\n",
    "plt.scatter(y_test_ext, y_pred_ext_cv)\n",
    "plt.xlabel(\"Actual Sale Price\")\n",
    "plt.ylabel(\"Predicted Sale Price\")\n",
    "plt.title(\"Experiment 3: Actual vs Predicted Sale Price (Lasso Regression with LassoCV)\")\n",
    "plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
